
\chapter{Introduction}

\begin{itemize}
	\item Kubernetes abstracts away the hardware infrastructure and exposes your whole data-center as a single enormous computational resources.
	\item To reduce the number of problems that only show up in production, it would be ideal if applications could run in the exact same environment during development and in production so they have the exact same operating system, libraries, system configuration, networking environment, and everything else.
	\item Ideally, you want the developers to deploy applications themselves without knowing anything about the hardware infrastructure and without dealing with the ops team.
	\item container technologies
	\item When using containers, you can (and should) have one container for each application.
	\item Linux Namespaces
	\item Linux Control Groups (cgroups)
	\item Kubernetes enables you to run your software applications on thousands of computer nodes as if all those nodes were a single, enormous computer.
	\item Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster. This is done through environment variables.
	\item The IP address of the service stays constant, so clients can always connect to its containers, even when they're moved around the cluster.
	\item On any node where Kubernetes is deployed, Kubernetes can run the app immediately without any help from the sysadmins.
\end{itemize}


\section{Principles}

One of the most fundamental Kubernetes principles. Instead of telling Kubernetes exactly what actions it should perform, you're only declaratively changing the desired state of the system and letting Kubernetes examine the current actual state and reconcile it with the desired state. This is true across all of Kubernetes.

\section{Features}
\begin{itemize}
	\item service discovery
	\item auto scaling
	\item load balancing
	\item self-healing
	\item leader election
\end{itemize}

\section{Concepts}

\begin{itemize}
	\item master node
	\item Kubernetes Control Planes
	\item worker node
	\item ReplicationController: Generally, ReplicationControllers are used to replicate pods (that is, create multiple copies of a pod) and keep them running.
\end{itemize}


\section{Tools}
\begin{itemize}
	\item Minikube
	\item kops
\end{itemize}

\section{Kubernetes Cluster Architecture}

\begin{itemize}
	\item Each node runs Docker, the Kubelet and the kube-proxy. You'll interact with the cluster through the \textit{kubectl} command line client, which issues REST requests to the Kubernetes API server running on the master node.
\end{itemize}


\begin{question}
Where do we run the kubctl command?
\end{question}


\includegraphics[max width=\textwidth]{introduction/k8s_cluster_architecture.png}

\section{Examples of Commonly Used Commands}

\begin{lstlisting}[style=bash]
kubectl run <ControllerName> --iamge=<ImageName> --port=8080 --generator=run/v1
kubectl expose rc <ControllerName> --type=LoadBalancer --name <ServiceName>
kubectl api-resources
kubectl get pods -o wide
kubectl describe pod <PodName>
\end{lstlisting}

test
